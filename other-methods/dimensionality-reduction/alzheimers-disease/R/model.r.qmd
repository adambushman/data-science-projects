---
title: "PCA & Clustering | Alzheimer's disease data"
description: "Leveraging principal component analysis and clustering as exploratory analysis techniques in Alzheimer's disease attributes."
author: "Adam Bushman"
date: "11/19/2024"
format: 
    html:
        toc: true
        theme: simplex
        smooth-scroll: true
        embed-resources: true
execute:
    warning: false
    error: false
---


# Assignment Questions

## Name

>   What is your name? Include all team members if submitting as a group.

Adam Bushman [u6049169]; no group members.


## Perspective

>   From what perspective ar you conducting the analysis? (Who are you? / Who are you working for?)

I am statistician working with a group of researchers dedicated to better understanding of early Alzheimer's detection. Despite no cure for the disease, early detection and diagnosis is critical for proper treatment to slow and mitigate onset of symptoms.

## Question

>   What is your question?

Our research group has collected data from 333 individuals, some with cognitive impairment and others who are perfectly healthy. These attributes (columns) range from demographics to protein measurements to dementia scores.

The key question is:

>   What natural levels of alzheimer risk/exposure are isolated from available data?

This is would be a valuable insight since detection prioritiy, therapy and medications could be tailored to each level uniquely.

## Data set

>   Describe your dataset(s) including URL (if available)

Data sourced from Posit via their 
[{modeldata}](https://modeldata.tidymodels.org/reference/ad_data.html) package, accessed November 14th, 2024. Named *[Alzheimer's disease data](https://modeldata.tidymodels.org/reference/ad_data.html)*, the data were originally sourced from *Kuhn, M., Johnson, K. (2013) Applied Predictive Modeling, Springer*, which derived observations from a clinical study of 333 patients. Citation:

>   Craig-Schapiro R, Kuhn M, Xiong C, Pickering EH, Liu J, Misko TP, et al. (2011) Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for Alzheimer's Disease Diagnosis and Prognosis. PLoS ONE 6(4): e18850.


## Variables

>   What are your variables? Include variable type (binary, categorical, numeric). If you have many variables, you can list the most important and summarize the rest (e.g. important variables are... also, 12 other binary, 5 categorical...).

This data set features 131 columns. Two factors and the rest numeric.

One is a natural dependent variable, `Class`, indicating if the patient is symptomatic of cognitive impairment or healthy. Since this exercise is not concerned with predictive modeling, we'll largely ignore this variable. All remaining variables are independent.

It's not clear which are most important for clustering patients at risk of or impaired with Alzheimer's.

For a complete list of each feature and their data types, navigate to the [feature summary](#feature-summary).


## Model resonability

>   How are your variables suitable for your analysis method?

The variables are suitable for the analysis since there's 1) high dimensionality where PCA can really shine and 2) it's not immediately clear what relationships work together for segmenting patients.

Because there's an inherent class, a "ground-truth" that is binary, it may be that there's no number of clusters we're able to naturally discover just given the problem chosen. 


## Conclusions

>   What are your conclusions (include references to one or two CLEARLY INDICATED AND IMPORTANT graphs or tables in your output)?

Initial clustering of Alzheimer attributes across patients performed poorly, with values centering near zero with a hierarchical and partition-based clustering approach.

PCA appeared to be [very successful at reducing the dimensionality](#pca-plot). With just the first 5 principal components, we were able to capture 50% of the variance. With fewer than half of the principal components, 90% of the variance was captured.

Looking at the "ground-truth" variable `Class` in the context of PCA, it was clear the impaired vs control groups are not easily differentiated, even with the first two principal components. 

The final clustering with PCA results yielded even worse cluster "tightness" scores than before. This is counter-intuitive and perhaps indicative of some failed assumptions made on the data set itself (see below).

## Assumptions

>   What are your assumptions and limitations? Did you include any robustness checks?

A clear assumption was that impaired and control patients could be differentiated using this data set. It's possible, and honestly likely, that they are not reliably segmented using the attribute values included here.

Another assumption is that more than two clusters may be identified in the data. This is unsubstantiated since I hadn't the time to thoroughly test the number of clusters against the Hubert or D-index values. The fact that PCA worsened the performance of the clustering later on suggests that "3" is not a natural number of clusters. 


# Assignment Workflow
 
## Analysis Prep

### Loading packages

```{r}
library('tidyverse')        # Wrapper for many convenient libraries
library('modeldata')        # Contains data for the assignment
library('skimr')            # Quickly summarise data
library('gt')               # Render "great tables"

library('kernlab')          # Weighted kernal k-means
library('dataPreparation')  # Utilities for PCA prep
library('dendextend')       # Working with dendrograms
library('cluster')          # Generating silhouette measures
```

### Loading the data

We'll start off by referencing the "Alzeimer's Data" for the assignment from the `{modeldata}` package.

```{r}
ad_raw <- modeldata::ad_data        # Data for the assignment
```

With it loaded into the session, let's get a sense for what we're working with.

### Data set inspection

Normally, I like to get acquainted a data set. That means understanding what each column seeks to describe, confirming the granularity of the rows, and getting my arms around structure, completeness, distribution, etc. We'll certainly do some of that, but this data set has over 130 columns.

Posit, the company behind `{modeldata}`, included the following summary of features classes captured in the dataset:

>   *   Demographic characteristics such as age and gender
*   Apolipoprotein E genotype
*   Protein measurements of Abeta, Tau, and a phosphorylated version of Tau (called pTau)
*   Protein measurements of 124 exploratory biomarkers, and
*   Clinical dementia scores


Using the `{skimr}` package, we can get a comprehensive summary of the data.
:::{#feature-summary}
```{r}
skim(ad_raw)
```

:::

<br>

Initial observations include:

*   333 rows represent the number of patients in the clinical study
*   We have two factor variables: `Genotype` and `Class`
    *   In theory, the `male` variable could be cast as a factor
*   We then have over 100 variables that are all numeric and all complete. So far, we haven't a clear understanding of the interplay and importance of these data points


## Exploratory Data Analysis

Our goal is to learn about the interplay and imporance of the various features in this data set. We'll use and combine Principal Component and Clustering analysis techniques. 

We'll follow a typical implementation, where we cluster the raw data, perform PCA, and then implement the same clustering analysis on the PCA output. We'll then determine where the most meaningful interpretation is derived.

### Data Prep

In both clustering and PCA, it's important to have properly scaled variables. This ensures each variable is "on the same level" and not experiencing any bias from large numbers. The `{dataPreparation}` package helps with this tremendously.

```{r}
ad_numeric <- ad_raw |> select(where(is.numeric))
```

```{r}
scale_obj <- build_scales(data_set = ad_numeric)
ad_scaled <- fast_scale(data_set = ad_numeric, scales = scale_obj, verbose = TRUE)
```

With these variables properly processed, we can begin our analysis work.

### Clustering (phase 1)

Let's experiment with both a hierarchical & partition-based method for clustering. In this way, we can compare results and see how PCA may impact each individually.

#### Hierarchical Clustering

We'll use a standard Euclidean distance function and the *ward.D2* method. We can then "cut" the resulting dendrogram to get the number of classes.

```{r}
d <- dist(ad_scaled, method = "euclidean")
fit_h <- hclust(d, method = "ward.D2")

class_h <- cutree(fit_h, 3)
```

We generated 3 clusters. There's several ways to quantify similarity of observations to their cluster. One way is with "average silhouette width", where values near 1 indicate a tight cluster.

```{r}
sil_h <- silhouette(class_h, d)
avg_sil_w_h <- mean(sil_h[,3])

avg_sil_w_h
```

#### Partition Clustering

Let's now cluster using a partition method, like Kernal K-Means. 

```{r}
kk_fit <- kkmeans(as.matrix(ad_scaled), centers = 3)
```

We'll then calculate the average silhouette width. We can't use the same distance measure because we're in a kernel space that differs from Euclidean distance. Let's generate a new function to create a distance matrix suitable for 'silhouette()':

```{r}
get_kernel_dist <- function(data) {
    kernel_matrix <- kernelMatrix(rbfdot(sigma = 0.1), data)
    pseudo_dist_matrix <- as.dist(sqrt(outer(diag(kernel_matrix), diag(kernel_matrix), "+") - 2 * kernel_matrix))

    return(pseudo_dist_matrix)
}
```

```{r}
d_p <- get_kernel_dist(as.matrix(ad_scaled))
sil_p <- silhouette(kk_fit, d_p)
avg_sil_w_p <- mean(sil_p[,3])

avg_sil_w_p
```

Both clustering algorithms yield nearly the same result: **~0.09**. On a scale of -1 to +1, these clustering algorithms aren't finding three, tight clusters. 

Let's perform PCA and then retry.


### Principal Component Analysis

#### Calculation

We'll use the scaled data, calculate covariance, and then use eigenvalues & eigenvectors to generate the variance explained by each principal component.

```{r}
ad_cov <- cov(ad_scaled)
ad_eig <- eigen(ad_cov)

ad_eig_val <- ad_eig$values
ad_eig_vec <- ad_eig$vectors
```

```{r}
var_expl <- round(ad_eig_val / sum(ad_eig_val), 3)
cum_var_expl <- cumsum(var_expl)
```

```{r}
pca_results <- data.frame(
    var = cum_var_expl, 
    idx = 1:length(cum_var_expl)
)

thresh <- c(0.5, 0.75, 0.9, 0.95)
idx <- sapply(thresh, function(t) which.min(abs(cum_var_expl - t)))

pca_thresh <- data.frame(thresh, idx)
```

#### Results Plots

:::{#pca-plot}
```{r}
ggplot() +
    geom_area(aes(x = idx, y = var), pca_results, fill = "#E2E6E6") +
    geom_vline(aes(xintercept = idx), pca_thresh, color = "#BE0000") +
    geom_label(
        aes(
            x = idx, y = thresh, 
            label = stringr::str_wrap(
                paste("First", idx, "of", length(cum_var_expl), "principal components explain", thresh * 100, "% of overall variance"), 20
            )
        ), 
        pca_thresh, 
        size = 2, 
        color = "#BE0000"
    ) +
    scale_y_continuous(expand = expansion(mult = c(0,.05))) +
    labs(
        title = "Cumulative variance explained by first X principal component(s)", 
        subtitle = paste0("A summary of PCA results compared to original column dimensions (", length(cum_var_expl), ")"), 
        x = "Principal Component Index", 
        y = "% of Variance Explained"
    ) +
    theme_minimal()
```
:::

We could also generate a plot comparing the Alzheimer `class` against the first two principal components and the final two. Let's get all principal components first and then organize those relevant into a data frame we can easily plot

```{r}
pca_scores <- ad_scaled * ad_eig_vec
```

```{r}
pca_groups <- tibble(
    class = ad_raw$Class, 
    pc_1 = unlist(pca_scores[,1], use.names = FALSE), 
    pc_2 = unlist(pca_scores[,2], use.names = FALSE), 
    pc_114 = unlist(pca_scores[,114], use.names = FALSE), 
    pc_115 = unlist(pca_scores[,115], use.names = FALSE)
)
```

Let's now generate the graphs. Hopefully, there's some visual separation class. Additionally, we would expect to see more distinct separation between class with 1st/2nd dimensions vs 114th/115th.

```{r}
ggplot(pca_groups) +
    geom_point(
        aes(pc_1, pc_2, color = class)
    ) +
    labs(
        title = "Comparison of principal components by `Class`", 
        subtitle = "Among 1st and 2nd principal components", 
        x = paste0("Dimension 1 (", var_expl[1] * 100, "%)"), 
        y = paste0("Dimension 2 (", var_expl[2] * 100, "%)")
    ) +
    theme_minimal()
```

```{r}
ggplot(pca_groups) +
    geom_point(
        aes(pc_114, pc_115, color = class)
    ) +
    labs(
        title = "Comparison of principal components by `Class`", 
        subtitle = "Among 114th and 115th principal components", 
        x = paste0("Dimension 114 (", var_expl[114] * 100, "%)"), 
        y = paste0("Dimension 115 (", var_expl[115] * 100, "%)")
    ) +
    theme_minimal()
```

The plots look very similar. Even with this dimensionality reduction and using the "ground-truth" class, we're not seeing a lot of power here. That's likely to limit our ability to improve clustering in phase 2.

### Clustering (phase 2)

Let's use the PCA values and perform the same clustering from before. We're looking for values better than ~0.09.

#### Hierarchical

```{r}
d2 <- dist(pca_scores, method = "euclidean")
fit_h2 <- hclust(d, method = "ward.D2")
class_h2 <- cutree(fit_h2, 3)

sil_h2 <- silhouette(class_h2, d2)
avg_sil_w_h2 <- mean(sil_h2[,3])

avg_sil_w_h2
```

#### Partition Clustering

```{r}
kk_fit2 <- kkmeans(as.matrix(pca_scores), centers = 3)

d_p2 <- get_kernel_dist(as.matrix(pca_scores))
sil_p2 <- silhouette(kk_fit2, d_p2)
avg_sil_w_p2 <- mean(sil_p2[,3])

avg_sil_w_p2
```

## Results

We're seeing worse results in clustering after PCA. This is counter intuitive as PCA should reduce the noise to which clustering may be suceptible.

It is possible that a value of "3" is clearly *not* the right number of clusters and PCA reinforces this takeaway. 