---
title: "ARIMA, Time Series Forecasting | Medicare Perscription Volumes"
description: "Forecasting the monthly volume of medicare perscription dispenses using the ARIMA model"
author: "Adam Bushman"
date: "10/15/2024"
format: 
    html:
        toc: true
        theme: simplex
        smooth-scroll: true
        embed-resources: true
execute:
    warning: false
    error: false
---

# Assignment Questions

## Name
>   What is your name? Include all team members if submitting as a group.

Adam Bushman [u6049169]; no group members.


## Perspective
>   From what perspective are you conducting the analysis? (Who are you? / Who are you working for?)

I am a medicare perscription plan manager, in charge of monitoring script dispenses across medicare plans. Its important for me to understand the anticipated scale (in volume and cost) of scripts for the next calendar year. 

## Question
>   What is your question?

What are the range of likely scenarios for monthly medicare perscription dispenses over the next 12 months?

## Data set
>   Describe your dataset(s) including URL (if available)

The data set is titled [*Monthly Medicare Australia prescription data*](https://tsibbledata.tidyverts.org/reference/PBS.html), sourced from the `{tsibbledata}` package. It was originally sourced from *Medicare Australia*.

## Predictor(s) and target(s)
>   What is (are) your independent variable(s) and dependent variable(s)? Include variable type (binary, categorical, numeric).

Independent variables are `Month`; other covariates are excluded for simplicity. This value is a of `month` data type, representing the unique month and year.

Dependent variables are `Scripts` and `Cost`, each forecasted indpendently. They are of type `double`.

Additional detail may be seen in the [data dictionary](#data-dictionary).

## Model resonability
>   How are your variables suitable for your analysis method?

The data are already formatted for use in time series analysis and forecasting. A `tsibble` is a modified version of a `tseries` but adheres to "tidy" principles. Additionally, the variables are well suited for use in ARIMA since we have consistent time intervals and continuous target variables. Furthermore, the data present opportunities to leverage the strengths of ARIMA, such as "non-stationarity" and "auto-correlation".

## Conclusions
>   What are your conclusions (include references to one or two CLEARLY INDICATED AND IMPORTANT graphs or tables in your output)?

While exploring monthly `Script` and `Cost` data, it was found that the delivered format was demonstrative of [seasonality and non-stationarity](#trend-plot), elements well-suited for ARIMA to correct. Having met the assumptions of the forecasting model, we proceeded to fit the model.

The `auto.arima` modeling approach yielded the following parameters:

*   $d = 2$: "differencing" the values was done twice to achieve stationarity
*   $q = 1$: only 1 previous value's error was included
*   $p = 2$: two lagged values were leveraged

The resulting model yielded [forecasted values](#forecasted-values) for the next twelve months, with an 80% confidence interval. The volume of scripts in the first forecasted month are anticipated to fall between 12.4M and 16.2M with 80% certainty.

## Assumptions
>   What are your assumptions and limitations? Did you include any robustness checks?

A big limitation in the data are the [non-normally distributed values](#distribution-1) of `Cost` and `Scripts`. [Logarithmic transforms](#distribution-2) were extremely helpful in resolving the issue but complicated interpretation of the model and forecasted values throughout. I'd be interested to learn more how this is handled in industry.

Additionally, the data was presented with variables levels and grouped covariates for `Concession`, `Type`, and `ATC1/2`. It's likely a medicare perscription program manager would be more interested in the varying forcasts that make up these slices. Due to knowledge gap in this area, it was resolved via aggregation.

It was also assumed that `auto.arima` would choose an optimal model. However, there was some suggestive evidence different values could have produced a similarly performant result but with more robustness. Ultimately, I ceded model choice to the algorithm. I could have run both and compared AICs.

# Assignment Workflow
 
## Analysis Prep

### Loading packages
```{r}
library('tidyverse')        # Wrapper for many convenient libraries
library('tsibble')          # Working with time series tibbles
library('tsibbledata')      # Contains data for the assignment
library('skimr')            # Quickly summarise data
library('gt')               # Render "great tables"

library('forecast')
library('tseries')
library('astsa')
```

### Loading the data
We'll begin by referencing the data from `{tsibbledata}`; the "PBS" data are the collection of variables respective to Australian medicare perscriptions.

```{r}
px_data <- tsibbledata::PBS          # Load data into the session
```

Let's now get a sense for what we're working with.

### Data set inspection
Right away, I like to get acquainted with the data set. That means understanding what each column seeks to describe, confirming the granularity of the rows, and getting my arms around structure, completeness, distribution, etc.

The package designers for `{tsibble}` included the following data dictionary:

```{r}
#| include: false

px_data_dict <- tibble(
    variable = c("Month", "Concession", "Type", "ATC1", "ATC1_desc", "ATC2", "ATC2_desc", "Scripts", "Cost"), 
    datatype = c("<yearmonth>", "<char>", "<char>", "<char>", "<char>", "<char>", "<char>", "<double>", "<double>"), 
    description = c(
        "A key for the year/month of the observation", 
        "Concessional scripts are given to pensioners, unemployed, dependents, and other card holders", 
        "Co-payments are made until an individual's script expenditure hits a threshold ($290.00 for concession, $1,141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount", 
        "Code for Anatomical Therapeutic Chemical index (level 1)", 
        "Description for Anatomical Therapeutic Chemical index (level 1); divides drugs into 14 anatomical groups based on the primary part of the body or organ system they act upon", 
        "Code for Anatomical Therapeutic Chemical index (level 2)", 
        "Code for Anatomical Therapeutic Chemical index (level 2); specifies the type of treatment or function the drugs perform within the organ system", 
        "Total number of scripts", 
        "Cost of the scripts in $AUD"
    )
)
```

::: {#data-dictionary}
```{r}
#| html-table-processing: none

gt(px_data_dict) %>%                                   # Create a "great tables" (gt) object
    cols_label(                                         # Rename some columns
        variable = "Variable Name", 
        datatype = "Data Type", 
        description = "Variable Description"
    ) %>%
    tab_options(
        column_labels.background.color = "#d9230f",     # Apply red header background
        column_labels.font.weight = "bold",             # Bold headers
        row.striping.background = '#FFFFFF'             # Remove row striping
    )
```
:::

Using the `{skimr}` package, we can get a comprehensive summary of the data.

```{r}
skim(px_data[,-1])          # Left out <yearmonth> data type
```
<br>

The data are mostly complete, thankfully. We see about 5% of records with missing `ATC1_desc` and `ATC2_desc`. These values aren't strictly necessary for a simple forecasting exercise anyway.

I always like to see the data using the `glimpse()` function from `{dplyr}`:

```{r}
head(px_data) |>
    mutate(Month = yearmonth(Month))
```

## Simple Exploratory Data Analysis

### Tracking levels
It's clear that the data are disaggregated. For example, the number of `Scripts` and total of `Cost` has been pieced up for the supplemental variables. Let's get a sense for what that looks like.

#### `Concession`
```{r}
px_data %>%
    count(.by = Concession) %>%
    mutate(share = n / sum(n))
```

#### `Type`
```{r}
px_data %>%
    count(.by = Type) %>%
    mutate(share = n / sum(n))
```

#### `ATC1`
```{r}
px_data %>%
    count(.by = ATC1) %>%
    mutate(share = n / sum(n))
```

#### Conclusion
This forecasting exercise is supposed to be simple. We aren't meant to generate multiple, grouped forecasts. We'll assume a slice or fully aggregated data set is of interest to the user.

### Assessing distribution
The variables available to us for forecasting are `Scripts` and `Cost`. Let's get a sense for their distribution. 

First, to make plotting easier, let's pivot the data to a long format:

```{r}
px_pivot <- px_data |> 
    as_tibble() |>
    select(Scripts, Cost) |>        # Restrict data to variables of interest        
    pivot_longer(                   # Pivot to a long version
        cols = everything(),                        
        names_to = "measure", 
        values_to = "value"
    )

head(px_pivot)
```

A great way to do visualize distribution is with a "raincloud" plot, which combines a traditional boxplot, scatter points, and a density shape. In this way, a complete understanding is had over the distribution of values. The below code is adopted from [Cedric Scherer's work](https://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/).

:::{#distribution-1}
```{r}
ggplot(                                                                         # Setup ggplot object
    px_pivot, 
    aes(x = "1", y = value)
) +                                    
    ggdist::stat_halfeye(                                                       # Distribution plot
        adjust = 0.5, 
        width = 0.6, 
        .width = 0, 
        justification = -0.2, 
        point_color = NA
    ) +
    geom_boxplot(                                                               # Boxplot
        width = 0.15, 
        outlier.shape = NA
    ) +
    gghalves::geom_half_point(                                                  # Individual observations
        side = "l", 
        range_scale = 0.4, 
        alpha = 0.2
    ) + 
    facet_wrap(~measure, nrow = 1) +                                            # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```
:::

Clearly the data are highly skewed. This opens up the possibility for a logarithmic transformation. Let's run the same code by using a log scale:

:::{#distribution-2}
```{r}
#| eval: true

ggplot(                                                                         # Setup ggplot object
    px_pivot, 
    aes(x = "1", y = value)
) +                                    
    ggdist::stat_halfeye(                                                       # Distribution plot
        adjust = 0.5, 
        width = 0.6, 
        .width = 0, 
        justification = -0.2, 
        point_color = NA
    ) +
    geom_boxplot(                                                               # Boxplot
        width = 0.15, 
        outlier.shape = NA
    ) +
    gghalves::geom_half_point(                                                  # Individual observations
        side = "l", 
        range_scale = 0.4, 
        alpha = 0.2
    ) + 
    scale_y_log10() +  
    facet_wrap(~measure, nrow = 1) +                                            # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```
:::

That's much improved! Such a transform may be a great way to improve our eventual forecasting model and better align with the assumptions of ARIMA.

### Trends

This is a forecasting exercise. As such, we'd be remiss if not to plot the trend. Let's first fully aggregate the data and then perform another pivot for a long format:

```{r}
px_trend <-
    px_data |>
    as_tibble() |>
    summarise(
        Total_Scripts = sum(Scripts), 
        Total_Cost = sum(Cost), 
        .by = Month
    ) |>
    pivot_longer(
        cols = -Month, 
        names_to = "measure", 
        values_to = "values"
    )

head(px_trend)
```

Let's now plot the trend for both measures. We'll apply a logarithmic transform right away since we know both measures are highly skewed.

:::{#trend-plot}
```{r}
ggplot(
    px_trend, 
    aes(Month, values)
) +
    geom_line() +
    scale_y_log10() +  
    facet_wrap(~measure, ncol = 1, scales = "free") +                           # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.y = element_blank(), 
        axis.title.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```
:::

We've got some nice plots now. Clearly the values are not stationary but that's a matter for the next section as we prepare the data for forecasting.

## Preprocessing

### Data cleaning / Feature Engineering
As we've already assessed, there's no "cleaning" to perform on the data since everything we're going to use is complete. However, we do want to do some feature engineering. We're going to tackle a handful of issues/checks:

1.  We need to formally aggregate the data so we haven't multiple levels per point in time
2.  We need resolve the skewness; we've already shown how to do that with a logarithmic transform
3.  We need to confirm non-stationarity; an ARIMA model requires no trend
4.  We also need to assess "auto-correlation", so the model knows how much to error to use in the moving average and how many lagged points to include

#### Aggregation
```{r}
px_agg <-
    px_data |>
    as_tibble() |>
    summarise(
        Total_Scripts = sum(Scripts), 
        Total_Cost = sum(Cost), 
        .by = Month
    ) |>
    mutate(Month = yearmonth(Month)) |>
    tsibble(index = Month)

head(px_agg)
```

#### Resolve skewness
```{r}
px_norm <-
    px_agg |>
    mutate(across(
        Total_Scripts:Total_Cost, 
        ~log(.), 
        .names = "{.col}_log"
    ))

head(px_norm)
```

#### Stationarity
We confirm stationarity using the "Augmented Dickey-Fuller Test". We'll do it for each of our logarithmically transformed variables, `Total_Scripts_log` and `Total_Cost_log`:

```{r}
adf.test(px_norm$Total_Scripts_log, alternative = "stationary")
```

```{r}
adf.test(px_norm$Total_Cost_log, alternative = "stationary")
```

Are the data stationary? Well, we set the alternative hypothesis to be "stationary", therefore a statistically significant result from the test (very low p-value) would indicate they are. And that's what we're getting.

However, we can see some trend in the data. Let's difference the values and see the results.

```{r}
adf.test(diff(px_norm$Total_Scripts_log), alternative = "stationary")
```

```{r}
adf.test(diff(px_norm$Total_Cost_log), alternative = "stationary")
```

THere's "some" improvement in the `Total_Cost_log`, but its marginal. What if we plot, quickly:

```{r}
ggplot(
    px_norm |>
    mutate(across(
        c(Total_Cost_log, Total_Scripts_log), 
        ~difference(.)
    )) |>
        as_tibble() |>
        pivot_longer(
            cols = c(Total_Scripts_log, Total_Cost_log), 
            names_to = "measure", values_to = "values"
        ), 
    aes(Month, values)
) +
    geom_line() +
    facet_wrap(~measure, ncol = 1, scales = "free") +                           # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.y = element_blank(), 
        axis.title.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```

That looks A LOT more stationary. We would say that $d=1$ for each value is sufficient in an ARIMA model.

```{r}
px_diff <- 
    px_norm |>
    mutate(
        Total_Scripts_log_diff = difference(Total_Scripts_log), 
        Total_Cost_log_diff = difference(Total_Cost_log)
    )

head(px_diff)
```

#### Auto-correlation
Finally, let's resolve any auto-correlation issues. We can assess this in two ways:

```{r}
acf(px_diff$Total_Scripts_log_diff[-1])
```

```{r}
acf(px_diff$Total_Cost_log_diff[-1])
```

In these plots, we're seeing 1 significant correlation (the second bar from the left). For an ARIMA model, we'll go with $q=1$.

Next, let's look at partial auto-correlation to determine $p$:

```{r}
pacf(px_diff$Total_Scripts_log_diff[-1])
```

```{r}
pacf(px_diff$Total_Cost_log_diff[-1])
```

These plots show three (3) significant correlations so we would use $p=3$ in an ARIMA model.

Of course, these values are slightly fluid. We could run multiple models testing different values and comparing the resulting AIC. The model with the lowest AIC is the best performing.

## Forecasting model

### Auto-ARIMA

For this exercise, we're going to let ARIMA find the best model. It will go through the same process we just did, finding optimal valuse of $d, q, p$.

```{r}
px_Scripts_fit <- auto.arima(px_norm$Total_Scripts_log)
```

```{r}
px_Cost_fit <- auto.arima(px_norm$Total_Cost_log)
```

### Evaluation

Let's see how the models performed, starting with the forecast for `Scripts`. We can use the `checkresiduals()` function:

```{r}
checkresiduals(px_Scripts_fit)
```

Interestingly, the model chose $d=2$ (we thought $d=1$ was sufficient), $q=1$ (same as our analysis), and $p=2$ (we estimated $p=3$). Looking at the plots, the model is only fair.

We can take this model and plot the forecast:

```{r}
fitted_mod <- forecast(px_Scripts_fit, h=12, level = c(80))

autoplot(fitted_mod) #+
    #scale_y_continuous(trans = scales::transform_exp())
```

Remember, those y-axis values for total number of scripts are on a logarithmic scale. We can extract the forecasted values from the model and transform them back to a linear scale:

:::{#forecasted-values}
```{r}
tibble(
    lower = exp(fitted_mod$lower), 
    mean = exp(fitted_mod$mean), 
    upper = exp(fitted_mod$upper)
)
```
:::

We can interpret this as total scripts during the first forecasted month will fall between 12.4M and 16.2M with 80% certainty.