---
title: "Arima, Time Series Forecasting | _______"
description: ""
author: "Adam Bushman"
date: "10/15/2024"
format: 
    html:
        toc: true
        theme: simplex
        smooth-scroll: true
        embed-resources: true
execute:
    warning: false
    error: false
---

# Assignment Questions

## Name
>   What is your name? Include all team members if submitting as a group.

Adam Bushman [u6049169]; no group members.


## Perspective
>   From what perspective are you conducting the analysis? (Who are you? / Who are you working for?)


## Question
>   What is your question?


## Data set
>   Describe your dataset(s) including URL (if available)


## Predictor(s) and target(s)
>   What is (are) your independent variable(s) and dependent variable(s)? Include variable type (binary, categorical, numeric).



## Model resonability
>   How are your variables suitable for your analysis method?



## Conclusions
>   What are your conclusions (include references to one or two CLEARLY INDICATED AND IMPORTANT graphs or tables in your output)?


## Assumptions
>   What are your assumptions and limitations? Did you include any robustness checks?



# Assignment Workflow
 
## Analysis Prep

### Loading packages
```{r}
library('tidyverse')        # Wrapper for many convenient libraries
library('tsibble')          # Working with time series tibbles
library('tsibbledata')      # Contains data for the assignment
library('skimr')            # Quickly summarise data
library('gt')               # Render "great tables"

library('forecast')
library('tseries')
library('astsa')
```

### Loading the data
We'll begin by referencing the data from `{tsibbledata}`; the "PBS" data are the collection of variables respective to Australian medicare perscriptions.

```{r}
px_data <- tsibbledata::PBS          # Load data into the session
```

Let's now get a sense for what we're working with.

### Data set inspection
Right away, I like to get acquainted with the data set. That means understanding what each column seeks to describe, confirming the granularity of the rows, and getting my arms around structure, completeness, distribution, etc.

The package designers for `{tsibble}` included the following data dictionary:

```{r}
#| include: false

px_data_dict <- tibble(
    variable = c("Month", "Concession", "Type", "ATC1", "ATC1_desc", "ATC2", "ATC2_desc", "Scripts", "Cost"), 
    datatype = c("<yearmonth>", "<char>", "<char>", "<char>", "<char>", "<char>", "<char>", "<double>", "<double>"), 
    description = c(
        "A key for the year/month of the observation", 
        "Concessional scripts are given to pensioners, unemployed, dependents, and other card holders", 
        "Co-payments are made until an individual's script expenditure hits a threshold ($290.00 for concession, $1,141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount", 
        "Code for Anatomical Therapeutic Chemical index (level 1)", 
        "Description for Anatomical Therapeutic Chemical index (level 1); divides drugs into 14 anatomical groups based on the primary part of the body or organ system they act upon", 
        "Code for Anatomical Therapeutic Chemical index (level 2)", 
        "Code for Anatomical Therapeutic Chemical index (level 2); specifies the type of treatment or function the drugs perform within the organ system", 
        "Total number of scripts", 
        "Cost of the scripts in $AUD"
    )
)
```

::: {#data-dictionary}
```{r}
#| html-table-processing: none

gt(px_data_dict) %>%                                   # Create a "great tables" (gt) object
    cols_label(                                         # Rename some columns
        variable = "Variable Name", 
        datatype = "Data Type", 
        description = "Variable Description"
    ) %>%
    tab_options(
        column_labels.background.color = "#d9230f",     # Apply red header background
        column_labels.font.weight = "bold",             # Bold headers
        row.striping.background = '#FFFFFF'             # Remove row striping
    )
```
:::

Using the `{skimr}` package, we can get a comprehensive summary of the data.

```{r}
skim(px_data[,-1])          # Left out <yearmonth> data type
```
<br>

The data are mostly complete, thankfully. We see about 5% of records with missing `ATC1_desc` and `ATC2_desc`. These values aren't strictly necessary for a simple forecasting exercise anyway.

I always like to see the data using the `glimpse()` function from `{dplyr}`:

```{r}
head(px_data) |>
    mutate(Month = yearmonth(Month))
```

## Simple Exploratory Data Analysis

### Tracking levels
It's clear that the data are disaggregated. For example, the number of `Scripts` and total of `Cost` has been pieced up for the supplemental variables. Let's get a sense for what that looks like.

#### `Concession`
```{r}
px_data %>%
    count(.by = Concession) %>%
    mutate(share = n / sum(n))
```

#### `Type`
```{r}
px_data %>%
    count(.by = Type) %>%
    mutate(share = n / sum(n))
```

#### `ATC1`
```{r}
px_data %>%
    count(.by = ATC1) %>%
    mutate(share = n / sum(n))
```

#### Conclusion
This forecasting exercise is supposed to be simple. We aren't meant to generate multiple, grouped forecasts. We'll assume a slice or fully aggregated data set is of interest to the user.

### Assessing distribution
The variables available to us for forecasting are `Scripts` and `Cost`. Let's get a sense for their distribution. 

First, to make plotting easier, let's pivot the data to a long format:

```{r}
px_pivot <- px_data |> 
    as_tibble() |>
    select(Scripts, Cost) |>        # Restrict data to variables of interest        
    pivot_longer(                   # Pivot to a long version
        cols = everything(),                        
        names_to = "measure", 
        values_to = "value"
    )

head(px_pivot)
```

A great way to do visualize distribution is with a "raincloud" plot, which combines a traditional boxplot, scatter points, and a density shape. In this way, a complete understanding is had over the distribution of values. The below code is adopted from [Cedric Scherer's work](https://www.cedricscherer.com/2021/06/06/visualizing-distributions-with-raincloud-plots-and-how-to-create-them-with-ggplot2/).

```{r}
ggplot(                                                                         # Setup ggplot object
    px_pivot, 
    aes(x = "1", y = value)
) +                                    
    ggdist::stat_halfeye(                                                       # Distribution plot
        adjust = 0.5, 
        width = 0.6, 
        .width = 0, 
        justification = -0.2, 
        point_color = NA
    ) +
    geom_boxplot(                                                               # Boxplot
        width = 0.15, 
        outlier.shape = NA
    ) +
    gghalves::geom_half_point(                                                  # Individual observations
        side = "l", 
        range_scale = 0.4, 
        alpha = 0.2
    ) + 
    facet_wrap(~measure, nrow = 1) +                                            # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```

Clearly the data are highly skewed. This opens up the possibility for a logarithmic transformation. Let's run the same code by using a log scale:

```{r}
#| eval: true

ggplot(                                                                         # Setup ggplot object
    px_pivot, 
    aes(x = "1", y = value)
) +                                    
    ggdist::stat_halfeye(                                                       # Distribution plot
        adjust = 0.5, 
        width = 0.6, 
        .width = 0, 
        justification = -0.2, 
        point_color = NA
    ) +
    geom_boxplot(                                                               # Boxplot
        width = 0.15, 
        outlier.shape = NA
    ) +
    gghalves::geom_half_point(                                                  # Individual observations
        side = "l", 
        range_scale = 0.4, 
        alpha = 0.2
    ) + 
    scale_y_log10() +  
    facet_wrap(~measure, nrow = 1) +                                            # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.x = element_blank(), 
        axis.title.y = element_blank(), 
        axis.text.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```

That's much improved! Such a transform may be a great way to improve our eventual forecasting model and better align with the assumptions of ARIMA.

### Trends

This is a forecasting exercise. As such, we'd be remiss if not to plot the trend. Let's first fully aggregate the data and then perform another pivot for a long format:

```{r}
px_trend <-
    px_data |>
    as_tibble() |>
    summarise(
        Total_Scripts = sum(Scripts), 
        Total_Cost = sum(Cost), 
        .by = Month
    ) |>
    pivot_longer(
        cols = -Month, 
        names_to = "measure", 
        values_to = "values"
    )

head(px_trend)
```

Let's now plot the trend for both measures. We'll apply a logarithmic transform right away since we know both measures are highly skewed.

```{r}
ggplot(
    px_trend, 
    aes(Month, values)
) +
    geom_line() +
    scale_y_log10() +  
    facet_wrap(~measure, ncol = 1, scales = "free") +                           # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.y = element_blank(), 
        axis.title.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```

We've got some nice plots now. Clearly the values are not stationary but that's a matter for the next section as we prepare the data for forecasting.

## Preprocessing

### Data cleaning / Feature Engineering
As we've already assessed, there's no "cleaning" to perform on the data since everything we're going to use is complete. However, we do want to do some feature engineering. We're going to tackle a handful of issues/checks:

1.  We need to formally aggregate the data so we haven't multiple levels per point in time
2.  We need resolve the skewness; we've already shown how to do that with a logarithmic transform
3.  We need to confirm non-stationarity; an ARIMA model requires no trend
4.  We also need to assess "auto-correlation", so the model knows how much to error to use in the moving average and how many lagged points to include

#### Aggregation
```{r}
px_agg <-
    px_data |>
    as_tibble() |>
    summarise(
        Total_Scripts = sum(Scripts), 
        Total_Cost = sum(Cost), 
        .by = Month
    ) |>
    mutate(Month = yearmonth(Month)) |>
    tsibble(index = Month)

head(px_agg)
```

#### Resolve skewness
```{r}
px_norm <-
    px_agg |>
    mutate(across(
        Total_Scripts:Total_Cost, 
        ~log(.), 
        .names = "{.col}_log"
    ))

head(px_norm)
```

#### Stationarity
We confirm stationarity using the "Augmented Dickey-Fuller Test". We'll do it for each of our logarithmically transformed variables, `Total_Scripts_log` and `Total_Cost_log`:

```{r}
adf.test(px_norm$Total_Scripts_log, alternative = "stationary")
```

```{r}
adf.test(px_norm$Total_Cost_log, alternative = "stationary")
```

Are the data stationary? Well, we set the alternative hypothesis to be "stationary", therefore a statistically significant result from the test (very low p-value) would indicate they are. And that's what we're getting.

However, we can see some trend in the data. Let's difference the values and see the results.

```{r}
adf.test(diff(px_norm$Total_Scripts_log), alternative = "stationary")
```

```{r}
adf.test(diff(px_norm$Total_Cost_log), alternative = "stationary")
```

THere's "some" improvement in the `Total_Cost_log`, but its marginal. What if we plot, quickly:

```{r}
ggplot(
    px_norm |>
    mutate(across(
        c(Total_Cost_log, Total_Scripts_log), 
        ~difference(.)
    )) |>
        as_tibble() |>
        pivot_longer(
            cols = c(Total_Scripts_log, Total_Cost_log), 
            names_to = "measure", values_to = "values"
        ), 
    aes(Month, values)
) +
    geom_line() +
    facet_wrap(~measure, ncol = 1, scales = "free") +                           # Generate two plots
    theme_minimal() +                                                           # Theme styling
    theme(
        axis.title.y = element_blank(), 
        axis.title.x = element_blank(), 
        panel.grid.major.x = element_blank(), 
        strip.background = element_rect(fill = "#BE0000", color = NA), 
        strip.text = element_text(color = "white", face = "bold")
    )
```

That looks A LOT more stationary. We would say that $d=1$ for each value is sufficient in an ARIMA model.

```{r}
px_diff <- 
    px_norm |>
    mutate(
        Total_Scripts_log_diff = difference(Total_Scripts_log), 
        Total_Cost_log_diff = difference(Total_Cost_log)
    )

head(px_diff)
```

#### Auto-correlation
Finally, let's resolve any auto-correlation issues. We can assess this in two ways:

```{r}
acf(px_diff$Total_Scripts_log_diff[-1])
```

```{r}
acf(px_diff$Total_Cost_log_diff[-1])
```

In these plots, we're seeing 1 significant correlation (the second bar from the left). For an ARIMA model, we'll go with $q=1$.

Next, let's look at partial auto-correlation to determine $p$:

```{r}
pacf(px_diff$Total_Scripts_log_diff[-1])
```

```{r}
pacf(px_diff$Total_Cost_log_diff[-1])
```

These plots show three (3) significant correlations so we would use $p=3$ in an ARIMA model.

Of course, these values are slightly fluid. We could run multiple models testing different values and compare AIC values.

## Forecasting model

### Auto-ARIMA

For this exercise, we're going to let ARIMA find the best model. It will go through the same process we just did, finding optimal valuse of $d, q, p$.

```{r}
px_Scripts_fit <- auto.arima(px_norm$Total_Scripts_log)
```

```{r}
px_Cost_fit <- auto.arima(px_norm$Total_Cost_log)
```

### Evaluation

Let's see how the models performed, starting with the forecast for `Scripts`. We can use the `checkresiduals()` function:

```{r}
checkresiduals(px_Scripts_fit)
```

## Conclusion