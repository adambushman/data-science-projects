---
title: "Neural Network | Palmer Penguin Data"
description: "Leveraging neural network regression in R to predict species of penguin"
author: "Adam Bushman"
date: "11/25/2024"
format: 
    html:
        toc: true
        theme: simplex
        smooth-scroll: true
        embed-resources: true
execute:
    warning: false
    error: false
---


# Assignment Questions

## Name

>   What is your name? Include all team members if submitting as a group.

Adam Bushman [u6049169]; no group members.


## Perspective

>   From what perspective ar you conducting the analysis? (Who are you? / Who are you working for?)

I am a researcher working to classify some penguin specie observations as part of a research project on the heals of the Palmer Archipelago expedition from which the source data was collected.


## Question

>   What is your question? 

*Using the source data from the Palmer Archipelago, can our research group develop an accurate neural network model for classifying species of penguin?*

This is important to our research group in efforts to personalize protection efforts by species.


## Data set

>   Describe your dataset(s) including URL (if available)

Data sourced from Posit via their 
[{modeldata}](https://modeldata.tidymodels.org/index.html) package, accessed November 25th, 2024. The data was originally sourced from [{palmerpenguins}](https://allisonhorst.github.io/palmerpenguins/index.html) package. Described as "A data set from Gorman, Williams, and Fraser (2014) containing measurements from different types of penguins." Full citation below:

>   Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.



## Predictor(s) and target(s)

>   What is (are) your independent variable(s) and dependent variable(s)? Include variable type (binary, categorical, numeric).

The dependent variable (target) for this use-case to this situation is `species`, a categorical feature. The remaining variables are suitable as independent variables (predictors): 

*   Numeric: `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`
*   Categorical: `island` and `sex`

For a complete description of each feature, navigate to the [data dictionary](#data-dictionary).


## Model resonability

>   How are your variables suitable for your analysis method?

The variables chosen above are the extent of the dataset (all features used). The analysis method is for a neural network classification model. These variables and the analysis method are a natural pairing thanks to:

*   Multiclass classification
    *   There are likely to be some complex relationships to explain differences in `species`
*   Non-linear relationships
    *   


## Conclusions

>   What are your conclusions (include references to one or two CLEARLY INDICATED AND IMPORTANT graphs or tables in your output)?

#### Model effectiveness

...


## Assumptions

>   What are your assumptions and limitations? Did you include any robustness checks?

### Assumptions made throughout the analysis

...


### Robustness checks


# Assignment Workflow
 
## Analysis Prep

### Loading packages

```{r}
library('tidyverse')        # Wrapper for many convenient libraries
library('tidymodels')       # Wrapper for modeling libraries
library('modeldata')        # Contains data for the assignment
library('skimr')            # Quickly summarise data
library('gt')               # Render "great tables"

library('nnet')             # Loading neural network library
library('VIM')              # Nearest neighbors imputation
library('themis')           # Library for generating a balanced data set ROSE
library('caret')            # Miscellaneous modeling functions
```

### Loading the data

We'll start off by referencing the "Palmer Penguin" data for the assignment from the `{modeldata}` package.

```{r}
peng_raw <- modeldata::penguins        # Data for the assignment
```

With it loaded into the session, let's get a sense for what we're working with.

### Data set inspection

Right away, I like to get acquainted with the data set. That means understanding what each column seeks to describe, confirming the granularity of the rows, and getting my arms around structure, completeness, distribution, etc.

Posit, the company behind `{modeldata}`, did not include a data dictionary; however, the source package, `{palmerpenguins}`, does feature a data dictionary. It is included below:

```{r}
#| include: false

peng_data_dict <- tibble(
    variable = c("species", "island", "bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "sex"), 
    datatype = c("<factor>", "<factor>", "<numeric>", "<numeric>", "<integer>", "<integer>", "<factor>"), 
    description = c(
        "Penguin species (Adelie, Chinstrap, Gentoo)", 
        "Island in Palmer Archipelago, Antartica (biscoe, Dream, or Torgerssen)", 
        "Bill length (millimeters)", 
        "Bill depth (millimeters)", 
        "Flipper length (millimeters)", 
        "Body mass (grams)", 
        "Penguin sex (female, male)"
    )
)
```

::: {#data-dictionary}
```{r}
#| html-table-processing: none

gt(peng_data_dict) %>%                                  # Create a "great tables" (gt) object
    cols_label(                                         # Rename some columns
        variable = "Variable Name", 
        datatype = "Data Type", 
        description = "Variable Description"
    ) %>%
    tab_options(
        column_labels.background.color = "#d9230f",     # Apply red header background
        column_labels.font.weight = "bold",             # Bold headers
        row.striping.background = '#FFFFFF'             # Remove row striping
    )
```
:::

Using the `{skimr}` package, we can get a comprehensive summary of the data.

```{r}
skim(peng_raw)
```
<br>

Initial observations include:

*   We have 344 rows and 7 columns; while a fairly small dataset, our neural network should be able to fit pretty fast
*   We have a couple missing values, namely `sex` (11 missing observations) and all of our numerics (2 missing observations); this means we'll have to do some imputation
*   We won't really have any skewed distributions
*   All our variables appear to be in the right data type
*   There's not a significant imbalance in classes
    *   `sex` is about even
    *   `species` and `island` have roughly 45-35-20 balancing each; one robustness check we can make later is to balance `species` in the training data


## Simple Exploratory Data Analysis



## Preprocessing

### Data cleaning

We've already mentioned the need to impute some values, specifically with `sex`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`.

Given the narrow scope of this data and some finite categories like species and island, we should be able to impute using nearest neighbors instead of defaulting to the "mean" (in the case of the numerics) or "majority class" (in the case of `sex`). We'll use the `{VIM}` package and its `kNN()` function:

```{r}
peng_imputed <- kNN(
    peng_raw, 
    variable = c("sex", "bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"), 
    k = 5
)
```

What we've said here is we're going to impute missing values for the 5 variables using the 5 nearest neighbors to each missing value. Let's double check `peng_imputed` has no missing values:

```{r}
sapply(peng_imputed, function(x) sum(is.na(x))) 
```

Perfect! Now, let's peak at the missing values for column:

```{r}
get_imputed <- function(x) {
    peng_imputed |> 
        filter({{ x }}) |>
        select(-c(sex_imp:body_mass_g_imp))
}
```

```{r}
print(get_imputed(sex_imp))
```

Looks like this predicted 4 `male` and 7 `female`. These missing values were all from two species and mostly two islands. A nearest neighbor approach in the data was a good idea since the rate of missing values wasn't consistent across all groups and distributions.

```{r}
print(get_imputed(bill_length_mm_imp))
```

We imputed two missing values for `bill_length_mm`. These observations were from completely diferent species and islands. The mean value we may have imputed is **43.9**, which likely would have lost some relationship value. The conditional mean by species at **38.8** and **47.5** do confirm our spread is about right. Unknown if that would have been a better value. We could test a model's sensitivity to this imputation later.

We'll remove the flags and save the variables:

```{r}
peng_clean <- peng_imputed |> select(-c(sex_imp:body_mass_g_imp))
```

### Balancing

We don't have "significant" imbalance in our target class, but a 45-35-20 isn't ideal either. It's possible a neural network won't be sensitive to the imbalance. We'll test this as part of our robustness checks so we'll generate another set of data balanced with *Synthetic Minority Oversampling Technique* or SMOTE.

We'll save this for later when we generate a "recipe". We'll use the `{themis}` package via `step_smote()`.

```{r}
#| eval: false
set.seed(819)                                           # Set reproducible seed
idx = createDataPartition(                              # Creat particion for the SMOTE data
    peng_imputed$species, 
    p = 0.7, 
    list = FALSE
)
peng_smote_test_imb <- peng_imputed[-idx,]              # Imbalanced testing data
peng_smote_train_imb <- peng_imputed[idx,]              # Imbalanced training data
```

We're leaving 30% of the data imbalanced. This would be our pseudo "testing" set for the SMOTE data. Let's balance a pseudo "training" set.

Next, we preprocess the data via dummy encoding and then perform the smote balancing using the `{scutr}` package.

```{r}
#| eval: false
peng_train_dmy <- dummyVars("~ . -species", peng_smote_train_imb)
peng_train_data_dmy <- data.frame(predict(peng_train_dmy, peng_smote_train_imb))
peng_test_dmy <- dummyVars("~ . -species", peng_smote_test_imb)
peng_test_data_dmy <- data.frame(predict(peng_test_dmy, peng_smote_test_imb)) |>
    mutate(species = peng_smote_test_imb$species)

peng_smote_train_bal <- SCUT(
    peng_train_data_dmy |> mutate(species = peng_smote_train_imb$species), 
    "species", 
    undersample = undersample_kmeans, 
    usamp_opts = list(k = 7)
)
```

```{r}
#| eval: false
table(peng_smote_train_bal$species) |> prop.table()
```

Now, instead of a 45-35-20 split, we're at a nice 33-33-33 balance! Let's finalize this all into an object suitable for `{tidymodels}`:

```{r}
#| eval: false
smote_data <- peng_smote_train_bal |> bind_rows(peng_test_data_dmy)

smote_split <- manual_rset(
    splits = list(make_splits(list(
        analysis = 1:nrow(peng_smote_train_bal),
        assessment = (nrow(peng_smote_train_bal)+1):nrow(smote_data)),
        data = smote_data
    )), 
    ids = "SMOTE_Split"
)
```


## Model resources

### Splitting for training/testing sets

As is customary, we must split our data into training and testing sets. This will be done on the original, imbalanced data (we already split for SMOTE). We'll put aside the testing set and work with training until we're comfortable with our model definition. The `{rsample}` package has a lot of helpful functions for this workflow. In just a couple lines we get training and testing sets.

```{r}
split_obj <- initial_split(peng_clean, prop = 0.75)              # Split object

peng_train <- training(split_obj)                                # Split for training
peng_test <- testing(split_obj)                                  # Split for testing
```


## Training and tuning

### Recipe

We're going to practice using the `{tidymodels}` framework for this modeling exercise. There's a lot of neat features we can make use of, first of which is a "recipe". A recipe will define the inputs/outputs, as well as any preprocessing

```{r}
preg_recipe <- recipe(species ~ ., peng_train) |>
    step_dummy(island, sex)
```

The recipe now knows what our modeling formula is and is prepped for use of the training data. Next, we'll set up some cross validation.

Let's also generate another recipe that will feature the `step_smote()`.

```{r}
preg_recipe_02 <- recipe(species ~ ., peng_train) |>
    step_dummy(island, sex) |>
    step_smote(species, over_ratio = 0.855)
```


### Cross validation tuning

Cross validation is important to ensure the results of a single model aren't an outlier. We'd hate to get a singular amazing result by chance that didn't align with the true range of performance. For the neural net, we want to try different tuning values; for "nnet" specifically, we're going to tune `hidden_units` (the nodes in the hidden layer), `penalty` (amount of regularization), and `epochs` (or number of cycles the model will run during fitting). 

We'll generate a tuning grid for these hyperparameters (a table with the various combinations we can use for each model):

```{r}
peng_tune_grid <- grid_regular(
    hidden_units(), 
    penalty(), 
    epochs(), 
    levels = 4
)

head(peng_tune_grid)
tail(peng_tune_grid)
```

Let's now create our cross validation folds. We set `v = 5` because we want 5 folds:

```{r}
peng_folds <- rsample::vfold_cv(peng_train, v = 5)
```

### Model setup

Let's begin by setting up the model definition. We specify this model will be a classification neural network using `nnet` and tuning `hidden_units`, `penalty`, and `epochs`. 

```{r}
nn_mod01 <- mlp(
    hidden_units = tune(), 
    penalty = tune(), 
    epochs = tune()
) |>
    set_engine("nnet") |>
    set_mode("classification")

nn_mod01 |> translate()
```

## Model training

### Workflow configuration

We next define a workflow that will take all these pieces and run them cohesively. We have a 1) recipe, 2) cross validation folds, 3) model, and 4) a hyperparameter tuning grid. Let's integrate them.

```{r}
peng_wflw <-
    workflow() |>
    add_model(nn_mod01) |>
    add_recipe(preg_recipe)
```

Next, we'll tune the model like so (we'll setup parallel processing to make the tuning process go quicker):

```{r}
doParallel::registerDoParallel(cores = 3)

set.seed(2015)
peng_tune <-
    peng_wflw |>
    tune_grid(
        resamples = peng_folds, 
        grid = peng_tune_grid, 
        metrics = metric_set(roc_auc)
    )
```

The model will calculate *ROC/AUC* values which will help in determining the "best model" based on hyperparameters used.

### Tuning Evaluation

```{r}
results_tune <- collect_metrics(peng_tune) |> arrange(mean)

head(results_tune)
tail(results_tune)
```

```{r}
ggplot(
    results_tune |> 
        pivot_longer(
            c(hidden_units:epochs)
        ), 
    aes(value, mean)
) +
    geom_line() + 
    geom_point() +
    facet_wrap(~name, nrow = 1, scales = "free")
```


```{r}
peng_best <- peng_tune |> select_best(metric = "roc_auc")
peng_best
```

Let's see if we get anything different with balanced data. We need a new recipe:

```{r}
peng_wflw_02 <-
    workflow() |>
    add_model(nn_mod02) |>
    add_recipe(preg_recipe_02)
```

```{r}
peng_tune_02 <-
    peng_wflw_02 |>
    tune_grid(
        resamples = peng_folds, 
        grid = peng_tune_grid, 
        metrics = metric_set(roc_auc)
    )
```

The model will calculate *ROC/AUC* values which will help in determining the "best model" based on hyperparameters used.

### Tuning Evaluation

```{r}
results_tune <- collect_metrics(peng_tune) |> arrange(mean)

head(results_tune)
tail(results_tune)
```

```{r}
doParallel::registerDoParallel(cores = 3)

bal_results <- fit_resamples(
    peng_wflw_02, 
    resamples = peng_folds, 
    metrics = metric_set(roc_auc)
)
```

```{r}
collect_metrics(bal_results)
```