---
title: "Position/role classification of incoming NBA prospects"
description: "Leveraging multiple machine learning methods in R to cluster for player positions/roles and predict the same for incoming NBA prospects."
author: "Adam Bushman"
date: "12/12/2024"
format: 
    html:
        toc: true
        theme: simplex
        smooth-scroll: true
        embed-resources: true
execute:
    warning: false
    error: false
---


# Assignment Workflow

## Organizational Theory

## Data Prep

### Load Libraries & Data Set

```{r}
library("tidyverse")
library("tidymodels")
library("tidyclust")
library("dataPreparation")

library("kernlab") # Weighted kernal k-means
```

```{r}
#| eval: false
setwd("full-projects/nba-player-position-roles/R")
```

Import the data

```{r}
nba_stats <- read.csv("../nba-player-data.csv")
```

```{r}
glimpse(nba_stats)
```

```{r}
# Check for missing or empty values
na_summary <- sapply(nba_stats, function(col) {
    sum(is.na(col) | is.null(col) | col == "", na.rm = TRUE)
})

any(na_summary > 0)
```


### Scale Variables

```{r}
nba_numeric <- nba_stats |> select(where(is.numeric))
nba_ids <- nba_stats |> select(where(is.character) | c(profile_height_in, profile_weight_lbs))
```

```{r}
# Check for non-finite values
nonf_summary <- sapply(nba_numeric, function(col) {
    sum(!is.finite(col), na.rm = TRUE)
})

any(nonf_summary > 0)
```

Scale the numeric variables:

```{r}
nba_scaled <- scale(nba_numeric)
```

### Principal Component Analysis

Using the scaled variables, let's perform PCA. We'll try to cluster with and without PCA. We'll have both data set versions at our disposal for clustering.

```{r}
library("FactoMineR")
```

```{r}
nba_pca <- princomp(nba_scaled)
```

```{r}
summary(nba_pca)
```

We have 199 numeric, source variables so 199 principal components. The first 12 components explain 75% of variance. The first 36 explain 90% of the variance. The first 53 principal components explain 95%. That's just 1/4 of our original number of features explaining nearly all of the total variance. 

PCA did a good job at 1) reducing dimensionality and 2) eliminating any colinearity of features. 

Let's save our results:

```{r}
nba_pca_data <- as.data.frame(nba_pca$scores)
```

Let's visualize some of these principal components:

```{r}
ggplot(
    nba_pca_data,
    aes(Comp.1, Comp.2)
) +
    geom_point()
```


## Clustering

Let's try to cluster these observations.

Historically, basketball has used 5 positions. In modern times, this has been reduced to approximately 3. Let's use 3 as our minimum and 12 as our maximum. Let's try a few different clustering techniques.

```{r}
clustering_grid <- data.frame(
    clusters = 3:20
)
```

Against each of these, we can run different clustering algorithms and produce measures for the "within sum of squares". This will help us determine the proper number of clusters derived from the data.


### Partition Clustering

Let's setup a function to run a kmeans cluster for every number of cluster in the above grid and generate the respective performance metric:

```{r}
cluster_kmeans <- function(k, data) {
    fit <- kmeans(data, k)
    vals <- glance(fit)
    return(vals$tot.withinss)
}
```   


### Hierarchical Clustering

Let's do the same thing but for an hclust algorithm:

```{r}
cluster_hclust <- function(k, data) {
    # Run the algorithm
    model <- hier_clust(num_clusters = k, linkage_method = "complete")
    fit <- model |> fit(~., data = as.data.frame(data))

    wss <- fit |>
        sse_within() |>
        select(wss) |>
        unlist() |>
        sum()
    return(wss)
}
```

Let's now generate our clusters!

### Cluster Results

Let's map over the number of clusters and execute the respective algorithm.

```{r}
clustering_grid_01 <-
    clustering_grid |>
    mutate(
        kmeans = map(clusters, ~ cluster_kmeans(.x, nba_scaled)),
        hclust = map(clusters, ~ cluster_hclust(.x, nba_scaled))
    )
```

We can now plot these and find the "elbow", or the point of diminishing returns from an increasing the number of clusters.

```{r}
clustering_grid_01 |>
    pivot_longer(cols = -clusters) |>
    unnest(value) |>
    ggplot(aes(factor(clusters), as.numeric(value))) +
    geom_line(aes(color = name), group = 1) +
    facet_wrap(~name, ncol = 1, scales = "free")
```



Using kmeans clustering, looks like the elbow point right around 8. Let's see if we get different results using just the first 53 principal components. 

```{r}
clustering_grid_02 <-
    clustering_grid |>
    mutate(
        kmeans = map(clusters, ~ cluster_kmeans(.x, nba_pca_data[, 1:53])),
        hclust = map(clusters, ~ cluster_hclust(.x, nba_pca_data[, 1:53]))
    )
```

```{r}
clustering_grid_02 |>
    pivot_longer(cols = -clusters) |>
    unnest(value) |>
    ggplot(aes(factor(clusters), as.numeric(value))) +
    geom_line(aes(color = name), group = 1) +
    facet_wrap(~name, ncol = 1, scales = "free")
```

This approach indicates an elbow right around 7 clusters.


```{r}
# install.packages("fpc")
# library(fpc)

cl <- dbscan(nba_pca_data[, 1:53], eps = 12, MinPts = 100)

summary(cl)
```


```{r}
set.seed(814)
fit <- kmeans(nba_pca_data[, 1:53], 7)

nba_ids$cluster <- fit$cluster

nba_ids |>
    group_by(cluster) |>
    summarise(
        players = paste(sample(player_name, 5), collapse = ",")
    )
```

Let's test some players. We should see three different cluster assignments.

```{r}
nba_ids |> filter(
    player_name %in% c(
        "Victor Wembanyama",
        "Nikola Jokic",
        "Clint Capela",
        "Rudy Gobert'"
    )
)
```

We do! Capela and Gobert's style is similar, but Jokic and Wembanyama are each distinct in their own right. This despite similar heights and weights.

Let's try another. These three all have similar physical profiles, roles, and play styles. Let's see how they are clustered.

```{r}
nba_ids |> filter(
    player_name %in% c(
        "Jimmy Butler",
        "Jayson Tatum",
        "Jaylen Brown"
    )
)
```

We see they all fall into the same cluster! This gives some assurance that the clustering is capturing some of the inherent patterns.

What about all players who've historically been labeled "point guards". Each of these play so differently we should see completely different cluster assignments.

```{r}
nba_ids |> filter(
    player_name %in% c(
        "Collin Sexton",
        "Bruce Brown",
        "Stephen Curry",
        "Jose Alvarado"
    )
)
```

We do! This tracks from a contextual lens.

Let's look at at some of the most dissimilar players from a physical profile that have the same cluster.

```{r}
getMinMax <- function(data, cluster = 1) {
    data_f <- data[data$cluster == cluster, ]
    data_f$val <- (scale(data_f$profile_height_in) + scale(data_f$profile_weight_lbs)) / 2
    data_f <- data_f |> arrange(desc(val))

    return(c(
        data_f$player_name[nrow(data_f)],
        data_f$player_name[1]
    ))
}
```

```{r}
for (c in sort(unique(nba_ids$cluster))) {
    players <- getMinMax(nba_ids, c)
    print(paste(
        c, "- Min:", players[1],
        "| Max:", players[2]
    ))
}
```

Some of these make a lot of sense! Others would require rethinking positions, roles, styles. But that's the point!

Looks like a lot of this is volume unadjusted

How many assignments by cluster do we have?

```{r}
nba_ids |> count(cluster)
```